{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s34FtqAjfqOp",
    "outputId": "9974d25f-7e03-480b-f26c-74b77e8f99ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-j3zCL8ugbYW"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import copy\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import contextlib\n",
    "import spacy\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "mHPCvwGlgUMY",
    "outputId": "e64c0556-5a08-4468-8402-21a85d36245e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-01 23:59:05+00:00</td>\n",
       "      <td>Selamat pagi bro n sis. Lebih baik #stayathome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-01 23:57:34+00:00</td>\n",
       "      <td>Merinding rasanya ngeliat lautan manusia di Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-01 23:57:00+00:00</td>\n",
       "      <td>Walikota Jakarta pusat sebelumnya sudah mengin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-01 23:56:26+00:00</td>\n",
       "      <td>Khofifah ingin mengawali dg peta penyebaran co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-01 23:55:47+00:00</td>\n",
       "      <td>sebagai orang yg masih percaya bahwa covid mas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>2021-05-05 08:28:01+00:00</td>\n",
       "      <td>AS berencana untuk meminjam hampir $ 1,3 trili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>2021-05-05 08:27:35+00:00</td>\n",
       "      <td>Teman2 nakes kl ada jurnal ttg covid atau arti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>2021-05-05 08:26:19+00:00</td>\n",
       "      <td>Sir corona ki wajah se hmara buisness band ho ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>2021-05-05 08:25:07+00:00</td>\n",
       "      <td>#BREAKING: Filipina laporkan kasus harian Covi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>2021-05-05 08:23:57+00:00</td>\n",
       "      <td>Bagi Kerajaan Muhyiddin, COVID amatlah penting...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date                                              tweet\n",
       "0     2021-05-01 23:59:05+00:00  Selamat pagi bro n sis. Lebih baik #stayathome...\n",
       "1     2021-05-01 23:57:34+00:00  Merinding rasanya ngeliat lautan manusia di Ta...\n",
       "2     2021-05-01 23:57:00+00:00  Walikota Jakarta pusat sebelumnya sudah mengin...\n",
       "3     2021-05-01 23:56:26+00:00  Khofifah ingin mengawali dg peta penyebaran co...\n",
       "4     2021-05-01 23:55:47+00:00  sebagai orang yg masih percaya bahwa covid mas...\n",
       "...                         ...                                                ...\n",
       "9995  2021-05-05 08:28:01+00:00  AS berencana untuk meminjam hampir $ 1,3 trili...\n",
       "9996  2021-05-05 08:27:35+00:00  Teman2 nakes kl ada jurnal ttg covid atau arti...\n",
       "9997  2021-05-05 08:26:19+00:00  Sir corona ki wajah se hmara buisness band ho ...\n",
       "9998  2021-05-05 08:25:07+00:00  #BREAKING: Filipina laporkan kasus harian Covi...\n",
       "9999  2021-05-05 08:23:57+00:00  Bagi Kerajaan Muhyiddin, COVID amatlah penting...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang = pd.read_csv(\"drive/My Drive/Thesis/Data/slang-corpus.txt\", sep='\\s*,\\s*') \n",
    "slang.head(20)\n",
    "\n",
    "tweets= pd.read_csv(\"drive/My Drive/Thesis/Data/tw_data_may.csv\") \n",
    "tweets.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bx9tijgefgFm"
   },
   "outputs": [],
   "source": [
    "class Preprocessing(object):\n",
    "   \n",
    "    def __init__(self):\n",
    "        self.stopwords = stopwords.words('indonesian')\n",
    "        self.ignored_word = ['rt','sih', 'loh', 'psx', 'ini', 'itu','eh', 'nya','si', 'oh', 'yg', 'ya', 'malaysia']\n",
    "        self.append_stopwords(self.ignored_word)\n",
    "            \n",
    "    def lowercase(self, text):\n",
    "        \"\"\" Lowercase text \"\"\"\n",
    "        return text.lower()\n",
    "    \n",
    "    def append_stopwords(self, text):\n",
    "        \"\"\" Append Custom Stopword \"\"\"\n",
    "        for word in text:\n",
    "            self.stopwords.append(word)\n",
    " \n",
    "    def remove_newlines(self, text):\n",
    "        \"\"\" Remove newlines characters from text \"\"\"\n",
    "        return text.replace(\"\\n\", \"\")\n",
    "    \n",
    "   \n",
    "    def remove_tabs(self,text):\n",
    "        \"\"\" Remove tab characters from text \"\"\"\n",
    "        return text.replace(\"\\t\", \"\")\n",
    "    \n",
    "   \n",
    "    def remove_digits(self,text):\n",
    "        \"\"\" Remove digits from text \"\"\"\n",
    "        return ''.join([word for word in text if not word.isdigit()])\n",
    "    \n",
    "    def remove_url(self,text):\n",
    "        \"\"\" Remove urls from text \"\"\"\n",
    "        tokens = tweet_tokenizer.tokenize(text)\n",
    "        output_array = []\n",
    "        for word in tokens:\n",
    "            cleaned_word = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', word, flags=re.MULTILINE)\n",
    "            output_array.append(cleaned_word)\n",
    "        return \" \".join(output_array)\n",
    "        \n",
    "        \n",
    "    def replace_special_chars(self,text):\n",
    "        \"\"\" Remove special characters from text \"\"\"\n",
    "        return re.sub(r'\\W+', ' ', text)\n",
    "\n",
    "    \n",
    "    def remove_multiple_whitespaces(self,text):\n",
    "        \"\"\" Remove whitespaces from text \"\"\"\n",
    "        return re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "\n",
    "   \n",
    "    def remove_trailing_whitespaces(self,text):\n",
    "        \"\"\" Remove whitespaces from beggining and ending text \"\"\"\n",
    "        return text.strip()\n",
    "     \n",
    "    def lemmatize(self, text):\n",
    "        \"\"\" Lemmatize words in text \"\"\"          \n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer()\n",
    "        text = stemmer.stem(text)\n",
    "        return text\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\" Remove stopwords from text \n",
    "        \n",
    "        :param text: text\n",
    "        :return: filtered stopwords from text\n",
    "        \"\"\"\n",
    "        tokens = tweet_tokenizer.tokenize(text)\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in self.stopwords]\n",
    "        text = \" \".join(filtered_tokens)\n",
    "        return text\n",
    "    \n",
    "    def fix_slang(self, text, slang):\n",
    "        \"\"\" Fix slang words from text \n",
    "        \n",
    "        :param text: text\n",
    "        :return: filtered slang from text\n",
    "        \"\"\"\n",
    "        tokens = tweet_tokenizer.tokenize(text)\n",
    "\n",
    "        filtered_tokens = []\n",
    "        gotcha = 0\n",
    "\n",
    "        for token in tokens:\n",
    "            for index, row in slang.iterrows():\n",
    "                if token == row[\"slang\"]:\n",
    "                    filtered_tokens.append(row[\"formal\"])\n",
    "                    gotcha = 1\n",
    "        \n",
    "            if gotcha == 0:\n",
    "                filtered_tokens.append(token)\n",
    "            else:\n",
    "                gotcha = 0\n",
    "            \n",
    "        text = \" \".join(filtered_tokens)\n",
    "\n",
    "        return text\n",
    "       \n",
    "    def clean_text(self, text, slang):\n",
    "        \"\"\" Generate a clean textual content\"\"\"\n",
    "        text = self.remove_url(text)\n",
    "        text = self.lowercase(text)\n",
    "        text = self.remove_newlines(text)\n",
    "        text = self.remove_tabs(text)\n",
    "        text = self.remove_digits(text)\n",
    "        text = self.replace_special_chars(text)\n",
    "        text = self.remove_multiple_whitespaces(text)\n",
    "        text = self.remove_trailing_whitespaces(text)        \n",
    "        text = self.fix_slang(text, slang)\n",
    "        text = self.lemmatize(text)\n",
    "        text = self.remove_stopwords(text)        \n",
    "        return text\n",
    "    \n",
    "    def clean_text_ws(self, text, slang):\n",
    "        \"\"\" Generate a clean textual content\"\"\"\n",
    "        text = self.remove_url(text)\n",
    "        text = self.lowercase(text)\n",
    "        text = self.remove_newlines(text)\n",
    "        text = self.remove_tabs(text)\n",
    "        text = self.remove_digits(text)\n",
    "        text = self.replace_special_chars(text)\n",
    "        text = self.remove_multiple_whitespaces(text)\n",
    "        text = self.remove_trailing_whitespaces(text)        \n",
    "        text = self.fix_slang(text, slang)     \n",
    "        text = self.lemmatize(text)\n",
    "        return text\n",
    "    \n",
    "    def process_text(self, data, slang):\n",
    "        result_sentence=[]\n",
    "        result_sentence_ws=[]\n",
    "        result_tokens=[] \n",
    "        for sentence in data:\n",
    "            clean_sentence = self.clean_text(sentence, slang)\n",
    "            result_sentence.append(clean_sentence)\n",
    "        return  result_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuOztJQFf79k"
   },
   "outputs": [],
   "source": [
    "processed_data = tweets\n",
    "prep = Preprocessing()\n",
    "data_clean = prep.process_text(processed_data[\"tweet\"], slang)\n",
    "\n",
    "processed_data['clean_sentences'] = data_clean\n",
    "\n",
    "processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UW3kYVIyno5_"
   },
   "outputs": [],
   "source": [
    "processed_data.to_csv('/Data/tweet_processed.csv',  index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
